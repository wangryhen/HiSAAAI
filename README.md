# HiSTalk: Hierarchical Speech Feature-based Landmark Displacements for 3D Talking Head Animation
The official repository of the paper [HiSTalk: Hierarchical Speech Feature-based Landmark Displacements for 3D Talking Head Animation](https://arxiv.org/abs/2404.01647)

<p align='center'>
  <b>
    <a href="">Paper</a>
    | 
    <a href="https://anonymous.4open.science/r/HiSTalk_Anonymous-3FDD/README.md">Project Page</a>
    |
    <a href="https://anonymous.4open.science/r/HiSTalk_Anonymous-3FDD/README.md">Code</a> 
  </b>
</p> 
<!--https://wangryhen.github.io/HSFTalk.github.io/>
<!-- Colab notebook demonstration: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Egq0_ZK5sJAAawShxC0y4JRZQuVS2X-Z?usp=sharing) -->

  <p align='center'>  
    <img src='/paper_images/framework.svg' width='1000'/>
  </p>

Given a speech signal as input, our framework <strong>HiSTalk</strong> can generate realistic 3D talking heads through the Hierarchical Speech Features to Sparse Landmarks <strong>(HSF2S)</strong> module and the Sparse Landmarks to Dense Landmarks Displacements <strong>(S2D)</strong> module.

## TODO
- [x] **Release Arxiv paper.**
- [x] **Release Project Page.**
- [ ] **Release code. (Once the paper is accepted)**
- [ ] **Release Pre-trained Model. (Once the paper is accepted)**



## Citation	

```
@article{2024histalk,
  title={HiSTalk: Hierarchical Speech Feature-based Landmark Displacements for 3D Talking Head Animation},
  author={},
  year={2025},
  eprint={},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}
```


## Acknowledgement
<!-- Some code are borrowed from following projects:
* [SpeechFormer++](https://github.com/wyhsirius/LIA)
* [Learning Landmarks](https://github.com/OpenTalker/DPE)
* [EAT](https://github.com/yuangan/EAT_code)
* [PD-FGC](https://github.com/Dorniwang/PD-FGC-inference)
* [Wav2Lip](https://github.com/Rudrabha/Wav2Lip)
* [FOMM video preprocessing](https://github.com/AliaksandrSiarohin/video-preprocessing) -->

 Some code and some figures in the paper are inspired by:
* [SpeechFormer++](https://arxiv.org/pdf/2302.14638)
* [Learning Landmarks](https://arxiv.org/pdf/2306.01415)
* [S2D-Dec](https://arxiv.org/pdf/2105.07463)

The README.md template is borrowed from [SyncTalk](https://github.com/ziqiaopeng/SyncTalk)


Thanks for these great projects.
